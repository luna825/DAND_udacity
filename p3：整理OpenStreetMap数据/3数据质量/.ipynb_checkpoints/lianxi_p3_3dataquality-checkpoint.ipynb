{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#   8、使用蓝图的示例\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'chicago.osm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-306e10df94ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m  \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mosm_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"chicago.osm\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mstreet_type_re\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\S+\\.?$'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIGNORECASE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'chicago.osm'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env  python\n",
    "# -*-  coding:utf-8  -*-\n",
    "import  xml.etree.cElementTree  as ET\n",
    "from  collections  import  defaultdict\n",
    "import  re\n",
    "\n",
    "osm_file = open(\"chicago.osm\", \"r\")\n",
    "\n",
    "street_type_re = re.compile(r'\\S+\\.?$', re.IGNORECASE)\n",
    "street_types = defaultdict(int)\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if  m:\n",
    "        street_type = m.group()\n",
    "        \n",
    "        street_types[street_type] += 1\n",
    "        \n",
    "\n",
    "def print_sorted_dict(d):\n",
    "    keys = d.keys()\n",
    "    keys = sorted(keys, key=lambda s: s.lower())\n",
    "    for k in keys:\n",
    "        v = d[k]\n",
    "        print  \"%s: %d\"  % (k, v)\n",
    "        \n",
    "        \n",
    "def is_street_name(elem):\n",
    "    return (elem.tag == \"tag\") and (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def  audit():\n",
    "    for  event, elem  in  ET.iterparse(osm_file):\n",
    "        if  is_street_name(elem):\n",
    "            audit_street_type(street_types, elem.attrib['v'])\n",
    "    print_sorted_dict(street_types)\n",
    "    \n",
    "\n",
    "if  __name__ == '__main__':\n",
    "    audit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11、审查交叉字段的约束条件 auditing  a  cross-field  constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'cities.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e98a47e7d265>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[1;32mif\u001b[0m  \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0minput_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cities.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mskip_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0maudit_population_density\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'cities.csv'"
     ]
    }
   ],
   "source": [
    "def ensure_float(v):\n",
    "    if is_number(v):\n",
    "        return float(v)\n",
    "    \n",
    "def audit_population_density(input_file):\n",
    "    for row in input_file:\n",
    "        population = ensure_float(row['populationTotal'])\n",
    "        area = ensure_float(row['areaLand'])\n",
    "        population_desity = ensure_float(row['populationDensity'])\n",
    "        if population and  area and population_desity:\n",
    "            calculated_desity = population / area\n",
    "            if math.fabs(calculated_desity - population_density) > 10:\n",
    "                print \"Possibly bad population density for \",row['name']\n",
    "                \n",
    "if  __name__ == '__main__':\n",
    "    input_file = csv.DictReader(open(\"cities.csv\"))\n",
    "    skip_lines(input_file, 3)\n",
    "    audit_population_density(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 12、修正有效性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your task is to check the \"productionStartYear\" of the DBPedia autos datafile for valid values.\n",
    "The following things should be done:\n",
    "- check if the field \"productionStartYear\" contains a year\n",
    "- check if the year is in range 1886-2014\n",
    "- convert the value of the field to be just a year (not full datetime)\n",
    "- the rest of the fields and values should stay the same\n",
    "- if the value of the field is a valid year in the range as described above,\n",
    "  write that line to the output_good file\n",
    "- if the value of the field is not a valid year as described above, \n",
    "  write that line to the output_bad file\n",
    "- discard rows (neither write to good nor bad) if the URI is not from dbpedia.org\n",
    "- you should use the provided way of reading and writing data (DictReader and DictWriter)\n",
    "  They will take care of dealing with the header.\n",
    "\n",
    "You can write helper functions for checking the data and writing the files, but we will call only the \n",
    "'process_file' with 3 arguments (inputfile, output_good, output_bad).\n",
    "\"\"\"\n",
    "import csv\n",
    "import pprint\n",
    "import pandas as pd\n",
    "\n",
    "INPUT_FILE = 'autos.csv'\n",
    "OUTPUT_GOOD = 'autos-valid.csv'\n",
    "OUTPUT_BAD = 'FIXME-autos.csv'\n",
    "\n",
    "def process_file(input_file, output_good, output_bad):\n",
    "    data_good = []\n",
    "    data_bad = []\n",
    "    with open(input_file, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        header = reader.fieldnames\n",
    "        for  row  in  reader:\n",
    "            if  row['URI'].find(\"dbpedia.org\") < 0:\n",
    "                continue\n",
    "                \n",
    "            ps_year = row['productionStartYear'][:4]\n",
    "            try: # ues try/except  to filter  valid  items\n",
    "                ps_year = int(ps_year)\n",
    "                row['productionStartYear'] = ps_year\n",
    "                if (ps_year >= 1886) and (ps_year <= 2014):\n",
    "                    data_good.append(row)\n",
    "                else:\n",
    "                    data_bad.append(row)\n",
    "            except  ValueError: # non-numeric strings caught by exception\n",
    "                if ps_year == 'NULL':\n",
    "                    data_bad.append(row)\n",
    " \n",
    "    # This is just an example on how you can use csv.DictWriter\n",
    "    # Remember that you have to output 2 files\n",
    "    with open(output_good, \"w\") as good:\n",
    "        writer = csv.DictWriter(good, delimiter=\",\", fieldnames= header)\n",
    "        writer.writeheader()\n",
    "        for row in data_good:\n",
    "            writer.writerow(row)\n",
    "            \n",
    "    with open(output_bad, \"w\") as bad:\n",
    "        writer = csv.DictWriter(bad, delimiter=',', fieldnames= header)\n",
    "        writer.writeheader()\n",
    "        for  row  in  data_bad:\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "def test():\n",
    "\n",
    "    process_file(INPUT_FILE, OUTPUT_GOOD, OUTPUT_BAD)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2108.0\n",
       "1       1.0\n",
       "2       NaN\n",
       "3       NaN\n",
       "Name: productionStartYear, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('FIXME-autos.csv')['productionStartYear'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1989\n",
       "1    1969\n",
       "2    1957\n",
       "3    1959\n",
       "4    1936\n",
       "Name: productionStartYear, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('autos-valid.csv')['productionStartYear'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 14/审查准确率 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client.examples\n",
    "\n",
    "\n",
    "def skip_lines(input_file, skip):\n",
    "    for i in input_file:\n",
    "        country = row['country_label']\n",
    "        country = contry.strip()\n",
    "        if (country == \"NULL\") or (country == \"\"):\n",
    "            continue\n",
    "        if db.countries.find({\"name\":country}).count() != 1:\n",
    "            print \"Not found:\", country\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    input_file = csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  15、审查完整性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  19/审查均匀性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'cities3.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-dbc713b313af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m\u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0minput_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cities3.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mskip_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mcounts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"nulls\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"empties\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;34m\"arrays\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'cities3.csv'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pprint\n",
    "\n",
    "fieldname = \"wgs84_pos#lat\"\n",
    "minval = -90\n",
    "maxval = 90\n",
    "\n",
    "def skip_lines(input_file, skip):\n",
    "    for i in range(0, skip):\n",
    "        next(input_file)\n",
    "        \n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "def audit_float_field(v, counts):\n",
    "    v = v.strip()\n",
    "    if v == \"NULL\":\n",
    "        counts['nulls'] += 1\n",
    "    elif  v == \"\":\n",
    "        counts['empties'] += 1\n",
    "    elif is_array(v):\n",
    "        counts['arrays'] += 1\n",
    "    elif not is_number(v):\n",
    "        print \"Found non number:\", v\n",
    "    else:\n",
    "        v = float(v)\n",
    "        if not ((minval < v) and (v < maxval)):\n",
    "            print \"Found out of range value:\", v\n",
    "            \n",
    "\n",
    "if __name__ ==\"__main__\":\n",
    "    input_file = csv.DictReader(open(\"cities3.csv\"))\n",
    "    skip_lines(input_file, 3)\n",
    "    counts = {\"nulls\": 0, \"empties\": 0 ,\"arrays\": 0}\n",
    "    nrows = 0\n",
    "    for row in input_file:\n",
    "        audit_float_field(row[fieldname], counts)\n",
    "        nvows += 1\n",
    "        \n",
    "    print \"num cities:\", nrows\n",
    "    print \"nulls:\", counts['nulls']\n",
    "    print \"empties:\", counts['empties']\n",
    "    print \"arrays:\", counts['arrays']\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 课程6：习题集：数据质量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1、审核数据质量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "In this problem set you work with cities infobox data, audit it, come up with a\n",
    "cleaning idea and then clean it up. In the first exercise we want you to audit\n",
    "the datatypes that can be found in some particular fields in the dataset.\n",
    "The possible types of values can be:\n",
    "- NoneType if the value is a string \"NULL\" or an empty string \"\"\n",
    "- list, if the value starts with \"{\"\n",
    "- int, if the value can be cast to int\n",
    "- float, if the value can be cast to float, but CANNOT be cast to int.\n",
    "   For example, '3.23e+07' should be considered a float because it can be cast\n",
    "   as float but int('3.23e+07') will throw a ValueError\n",
    "- 'str', for all other values\n",
    "\n",
    "The audit_file function should return a dictionary containing fieldnames and a \n",
    "SET of the types that can be found in the field. e.g.\n",
    "{\"field1\": set([type(float()), type(int()), type(str())]),\n",
    " \"field2\": set([type(str())]),\n",
    "  ....\n",
    "}\n",
    "The type() function returns a type object describing the argument given to the \n",
    "function. You can also use examples of objects to create type objects, e.g.\n",
    "type(1.1) for a float: see the test function below for examples.\n",
    "\n",
    "Note that the first three rows (after the header row) in the cities.csv file\n",
    "are not actual data points. The contents of these rows should note be included\n",
    "when processing data types. Be sure to include functionality in your code to\n",
    "skip over or detect these rows.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'areaCode': set([<type 'int'>, <type 'str'>]),\n",
      " 'areaLand': set([<type 'float'>, <type 'list'>, <type 'str'>]),\n",
      " 'areaMetro': set([<type 'float'>, <type 'str'>]),\n",
      " 'areaUrban': set([<type 'float'>, <type 'str'>]),\n",
      " 'elevation': set([<type 'float'>, <type 'list'>, <type 'str'>]),\n",
      " 'governmentType_label': set([<type 'str'>]),\n",
      " 'homepage': set([<type 'str'>]),\n",
      " 'isPartOf_label': set([<type 'list'>, <type 'str'>]),\n",
      " 'maximumElevation': set([<type 'str'>]),\n",
      " 'minimumElevation': set([<type 'str'>]),\n",
      " 'name': set([<type 'list'>, <type 'str'>]),\n",
      " 'populationDensity': set([<type 'float'>, <type 'list'>, <type 'str'>]),\n",
      " 'populationTotal': set([<type 'int'>, <type 'str'>]),\n",
      " 'timeZone_label': set([<type 'str'>]),\n",
      " 'utcOffset': set([<type 'int'>, <type 'list'>, <type 'str'>]),\n",
      " 'wgs84_pos#lat': set([<type 'float'>]),\n",
      " 'wgs84_pos#long': set([<type 'float'>])}\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-896ca3c11d0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-896ca3c11d0e>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mpprint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfieldtypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[1;32massert\u001b[0m \u001b[0mfieldtypes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"areaLand\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mfieldtypes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"areaMetro\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import csv\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "CITIES = 'cities.csv'\n",
    "\n",
    "FIELDS = [\"name\",\"timeZone_label\", \"utcOffset\",\"homepage\", \"governmentType_label\", \n",
    "         \"isPartOf_label\", \"areaCode\", \"populationTotal\", \"elevation\", \n",
    "          \"maximumElevation\", \"minimumElevation\", \"populationDensity\",\n",
    "         \"wgs84_pos#lat\", \"wgs84_pos#long\", \"areaLand\", \"areaMetro\", \"areaUrban\"]\n",
    "\n",
    "def  skip_lines(input_file, skip):\n",
    "    for i in range(skip):\n",
    "        next(input_file)\n",
    "        \n",
    "        \n",
    "def is_int(v):\n",
    "    try:\n",
    "        int(v)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def  is_float(v):\n",
    "    try:\n",
    "        float(v)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "              \n",
    "\n",
    "def audit_file(filename, fields):\n",
    "    fieldtypes = {}\n",
    "    for field in fields:\n",
    "        fieldtypes[field] = set()\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        skip_lines(reader, 3)\n",
    "        for row in reader:\n",
    "            for field in fields:\n",
    "                v = row[field]\n",
    "                if v == \"Null\" or v == \"\":\n",
    "                    fieldtypes[field].add(type(None))\n",
    "                elif v.startswith('{'):\n",
    "                    fieldtypes[field].add(type([]))\n",
    "                elif is_int(v):\n",
    "                    fieldtypes[field].add(type(1))\n",
    "                elif is_float(v):\n",
    "                    fieldtypes[field].add(type(1.1))\n",
    "                else:\n",
    "                    fieldtypes[field].add(type('1.1'))\n",
    "\n",
    "    return fieldtypes\n",
    "\n",
    "\n",
    "def test():\n",
    "    fieldtypes = audit_file(CITIES, FIELDS)\n",
    "    \n",
    "    pprint.pprint(fieldtypes)\n",
    "    \n",
    "    assert fieldtypes[\"areaLand\"] == set([type(1.1), type([]), type(None)])\n",
    "    assert fieldtypes[\"areaMetro\"] == set([type(1.1), type(None)])\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3、修复区域"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "In this problem set you work with cities infobox data, audit it, come up with a\n",
    "cleaning idea and then clean it up.\n",
    "\n",
    "Since in the previous quiz you made a decision on which value to keep for the\n",
    "\"areaLand\" field, you now know what has to be done.\n",
    "\n",
    "Finish the function fix_area(). It will receive a string as an input, and it\n",
    "has to return a float representing the value of the area or None.\n",
    "You have to change the function fix_area. You can use extra functions if you\n",
    "like, but changes to process_file will not be taken into account.\n",
    "The rest of the code is just an example on how this function can be used.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import csv\n",
    "import json\n",
    "import pprint \n",
    "\n",
    "CITIES = 'cities.csv'\n",
    "\n",
    "def fix_area(area):\n",
    "    \n",
    "    if area == \"NULL\":\n",
    "        area = None\n",
    "    elif is_float(area):\n",
    "        area = float(area)\n",
    "    else:\n",
    "        area_parts = area.split(\"|\")\n",
    "        first_area = area_parts[0][1:]\n",
    "        second_area = area_parts[1][:(len(area_parts[1]) - 1)]\n",
    "        if len(first_area.split(\".\")[1]) > len(second_area.split(\".\")[1]):\n",
    "            area = float(first_area)\n",
    "        else:\n",
    "            area = float(second_area)    \n",
    "    \n",
    "    return area\n",
    "\n",
    "def is_float(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def process_file(filename):\n",
    "    # change to this function will be ignored when you submit the exercise\n",
    "    data = []\n",
    "    \n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        \n",
    "        #skipping the extra metadata\n",
    "        for i in range(3):\n",
    "            l = reader.next()\n",
    "            \n",
    "        # processing file\n",
    "        for line in  reader:\n",
    "            # calling your function to fix the area value\n",
    "            if \"areaLand\" in line:\n",
    "                line[\"areaLand\"] = fix_area(line[\"areaLand\"])\n",
    "            data.append(line)\n",
    "            \n",
    "    return data\n",
    "            \n",
    "def test():\n",
    "    data = process_file(CITIES)\n",
    "    \n",
    "    print \"Printing three example results:\"\n",
    "    for n in range(5, 8):\n",
    "        pprint.pprint(data[n][\"areaLand\"])\n",
    "        \n",
    "    assert data[3][\"areaLand\"] == None\n",
    "    assert data[8][\"areaLand\"] == 55166700.0\n",
    "    assert data[20][\"areaLand\"] == 14581600.0\n",
    "    assert data[33][\"areaLand\"] == 20564500.0\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5/修复姓名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "In this problem set you work with cities infobox data, audit it, come up with a\n",
    "cleaning idea and then clean it up.\n",
    "\n",
    "In the previous quiz you recognized that the \"name\" value can be an array (or\n",
    "list in Python terms). It would make it easier to process and query the data\n",
    "later if all values for the name are in a Python list, instead of being\n",
    "just a string separated with special characters, like now.\n",
    "\n",
    "Finish the function fix_name(). It will recieve a string as an input, and it\n",
    "will return a list of all the names. If there is only one name, the list will\n",
    "have only one item in it; if the name is \"NULL\", the list should be empty.\n",
    "The rest of the code is just an example on how this function can be used.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import csv\n",
    "import pprint \n",
    "\n",
    "CITIES = \"cities.csv\"\n",
    "\n",
    "def fix_name(name):\n",
    "    if name == \"NULL\":\n",
    "        name = []\n",
    "    elif name.startswith(\"{\"):\n",
    "        name = name.replace(\"{\",\"\").replace(\"}\",\"\").split(\"|\")\n",
    "    else:\n",
    "        name = [name] \n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "def process_file(filename):\n",
    "    data = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        #skipping the extra metadata\n",
    "        for i in range(3):\n",
    "            l = reader.next()\n",
    "        # processing file\n",
    "        for line in reader:\n",
    "            # calling your function to fix the area value\n",
    "            if \"name\" in line:\n",
    "                line[\"name\"] = fix_name(line[\"name\"])\n",
    "            data.append(line)\n",
    "    return data\n",
    "\n",
    "def test():\n",
    "    data = process_file(CITIES)\n",
    "    \n",
    "    print \"Printing 20 results:\"\n",
    "    for n in range(20):\n",
    "        pprint.pprint(data[n][\"name\"])\n",
    "        \n",
    "    assert data[14][\"name\"] == [\"Negtemiut\", \"Nightmute\"]\n",
    "    assert data[9][\"name\"] == ['Pell City Alabama']\n",
    "    assert data[3][\"name\"] == ['Kumhari']\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6/交叉字段审查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn this problem set you work with cities infobox data, audit it, come up with a\\ncleaning idea and then clean it up.\\n\\nIf you look at the full city data, you will notice that there are couple of\\nvalues that seem to provide the same information in different formats: \"point\"\\nseems to be the combination of \"wgs84_pos#lat\" and \"wgs84_pos#long\". However,\\nwe do not know if that is the case and should check if they are equivalent.\\n\\nFinish the function check_loc(). It will recieve 3 strings: first, the combined\\nvalue of \"point\" followed by the separate \"wgs84_pos#\" values. You have to\\nextract the lat and long values from the \"point\" argument and compare them to\\nthe \"wgs84_pos# values, returning True or False.\\n\\nNote that you do not have to fix the values, only determine if they are\\nconsistent. To fix them in this case you would need more information. Feel free\\nto discuss possible strategies for fixing this on the discussion forum.\\n\\nThe rest of the code is just an example on how this function can be used.\\nChanges to \"process_file\" function will not be taken into account for grading.\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "In this problem set you work with cities infobox data, audit it, come up with a\n",
    "cleaning idea and then clean it up.\n",
    "\n",
    "If you look at the full city data, you will notice that there are couple of\n",
    "values that seem to provide the same information in different formats: \"point\"\n",
    "seems to be the combination of \"wgs84_pos#lat\" and \"wgs84_pos#long\". However,\n",
    "we do not know if that is the case and should check if they are equivalent.\n",
    "\n",
    "Finish the function check_loc(). It will recieve 3 strings: first, the combined\n",
    "value of \"point\" followed by the separate \"wgs84_pos#\" values. You have to\n",
    "extract the lat and long values from the \"point\" argument and compare them to\n",
    "the \"wgs84_pos# values, returning True or False.\n",
    "\n",
    "Note that you do not have to fix the values, only determine if they are\n",
    "consistent. To fix them in this case you would need more information. Feel free\n",
    "to discuss possible strategies for fixing this on the discussion forum.\n",
    "\n",
    "The rest of the code is just an example on how this function can be used.\n",
    "Changes to \"process_file\" function will not be taken into account for grading.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pprint\n",
    "\n",
    "CITIES = 'cities.csv'\n",
    "\n",
    "def check_loc(point, lat, longi):\n",
    "    pointLat, pointLongi = point.split(\" \")[0], point.split(\" \")[1]\n",
    "    if pointLat == lat and pointLongi == longi:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_file(filename):\n",
    "    data = []\n",
    "    with open(filename,\"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        # skipping the extra matadata\n",
    "        for i in range(3):\n",
    "            l = reader.next()\n",
    "            \n",
    "        # processing file\n",
    "        for line in reader:\n",
    "            # calling your function to check the location\n",
    "            result = check_loce(line[\"point\"], line[\"wgs94_pos#lat\"], line[\"wgs84_pos#long\"])\n",
    "            if not result:\n",
    "                print \"{}:{} != {}{}\".format(line[\"name\"], line[\"point\"], line[\"wgs84_pos#lat\"], line[\"wgs84_pos#long\"])\n",
    "            data.append(line)\n",
    "            \n",
    "        return data\n",
    "\n",
    "\n",
    "def test():\n",
    "    assert check_loc(\"33.08 75.28\", \"33.08\", \"75.28\") == True\n",
    "    assert check_loc(\"44.57833333333333 -91.21833333333333\", \"44.5783\", \"-91.2183\") == False\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    test()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
